---
output:
  html_document:
    theme: readable
  pdf_document: default
---

```{r, echo = FALSE, message = FALSE}
library(lubridate)
date <- "04-06-2022"
weekday <- wday(mdy(date), label = TRUE, abbr = FALSE)
month <- month(mdy(date), label = TRUE)
day <- day(mdy(date))
```

---
title: `r paste(weekday, ", ", month, " ", day, sep = "")`
header-includes:
  - \usepackage{float}
  - \usepackage{booktabs}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, out.width = "100%", fig.width = 9, fig.align = "center", cache = FALSE, dev = ifelse(knitr::is_html_output(), "png", "pdf"))
```

```{r packages}
library(dplyr)
library(tidyr)
library(ggplot2)
library(kableExtra)
```

```{r utilities}
source("../../utilities.R")
```

`r ifelse(knitr::is_html_output(), paste("You can also download a [PDF](lecture-", date, ".pdf) copy of this lecture.", sep = ""), "")`


## Independent Versus Dependent Samples

**Example**: Consider a study like the twin study for investigating the relationship between schizophrenia and the volume of the left hippocampus.
```{r}
d <- Sleuth3::case0202
d$Difference <- with(d, Unaffected - Affected)
m <- round(mean(d$Difference), 2)
s <- round(sd(d$Difference), 2)
n <- nrow(d)
d$Pair = as.character(1:n)
d <- d[,c(4,1,2,3)]
d$Difference <- round(d$Difference,2)
tmp <- rbind(d, c("Size:", rep(15, 3)))
tmp <- rbind(tmp, c("Mean:", round(apply(d[,-1], 2, mean),3)))
tmp <- rbind(tmp, c("SD:", round(apply(d[,-1], 2, sd), 3)))
ktbl(tmp, align = "rccc") %>% add_header_above(c(" " = 1, "Schizophrenia" = 2, " " = 1))
```
How would we make inferences if the samples are *dependent*, and how would we make inferences if the samples are *independent*? 

\pagebreak

## Matching in Observational Studies

**Example**: Consider a study to compare the foot hair density of Hobbits who smoke pipe-weed with that of Hobbits that do not smoke. But we use an *observational* study and are concerned that smoking and foot hair density are also related to the confounding variables *age* and *Farthing*. We can control for age and Farthing by matching Hobbits based on those two variables. 
```{r}
n <- 50
set.seed(123)
d.n <- data.frame(Age = sample(30:130, n, replace = TRUE), 
  Farthing = sample(c("N","S","E","W"), n, replace = TRUE),
  FHDI = round(rnorm(n, 80, 10), 1))
d.s <- d.n[,c(3,2,1)]
d.s$Age <- d.s$Age + sample(-3:3, n, replace = TRUE)
d.s$FHDI <- round(rnorm(n, 70, 10), 1)
d <- cbind(1:(2*n), d.n, " ", d.s)
names(d)[1] <- "Pair"
names(d)[5] <- " "
d.head <- headtail(d,5)
d.head[6,5] <- " "
ktbl(d.head) %>% add_header_above(c(" " = 1, "Smoker" = 3, " " = 1, "Non-Smoker" = 3))
```
This creates a matched-pairs design with dependent samples.
```{r}
d <- d[,c(1,4,6)]
names(d)[2:3] <- c("Smoker","Non-Smoker")
d$Difference <- round(d[,2]-d[,3],1)
ktbl(headtail(d,5)) %>% add_header_above(c(" " = 1, "FHDI" = 2, " " = 1))
```

\pagebreak

## Standard Errors for Dependent Versus Independent Samples

Suppose we have two samples, both of equal size $n$ (so we'll leave off the subscript). The standard error of $\bar{x}_1-\bar{x}_2$ is
$$
	\underbrace{\sqrt{\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{n}}}_{\text{independent}} \ge 
	\underbrace{\sqrt{\frac{\sigma_1^2}{n} + \frac{\sigma_2^2}{n} - \rho\frac{2\sigma_1\sigma_2}{n}}}_{\text{dependent}} = \frac{\sigma_d}{\sqrt{n}}.
$$
Here $\sigma_d$ is the standard deviation of the *differences* of matched observations, and $\rho$ is the *correlation coefficient*. The correlation coefficient can be between -1 and 1. 

1. If the samples are *independent* then $\rho = 0$.
0. If the samples are *dependent* then (usually) $0 < \rho < 1$.

What does this imply about the standard errors for dependent versus independent samples? 

**Example**: Suppose we had a matched-pairs design using genetically-related individuals (e.g., cousins, siblings, or identical twins) for a study like that that investigated the relationship between schizophrenia and left hippocampus volume. 
```{r, fig.width = 9}
rho <- function(v, rho) {
  y <- diag(v)
  for (i in 1:nrow(y)) {
    for (j in 1:ncol(y)) {
      if (i != j) {
        y[i,j] <- sqrt(y[i,i]) * sqrt(y[j,j]) * rho   
      }
    }
  }
  return(y)
}

n <- 100

set.seed(101)

m <- apply(Sleuth3::case0202, 2, mean)
v <- apply(Sleuth3::case0202, 2, var)

y1 <- MASS::mvrnorm(n, m, rho(v,0.3))
y2 <- MASS::mvrnorm(n, m, rho(v,0.8))
y3 <- MASS::mvrnorm(n, m, rho(v,0.95))

d <- data.frame(rbind(y1,y2,y3))
names(d) <- c("x1","x2")
d$r <- rep(c(0.0,0.7,0.9), each = n)
d$r <- factor(d$r, labels = paste("$\\rho=", unique(d$r), "$", sep = ""))
levels(d$r) <- c("Low Dependence","Medium Dependence","High Dependence")

p <- ggplot(d, aes(x = x1, y = x2)) + facet_wrap(~r)
p <- p + geom_point(size = 0.5) + coord_fixed()
p <- p + labs(x = "Unaffected Twin Volume", y = "Affected Twin Volume")
p <- p + theme_minimal()
plot(p)
```

\pagebreak

## The Other Standard Error for $\bar{x}_1-\bar{x}_2$

If two samples are independent, the standard error we have been using is
$$
  \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}.
$$
An alternative is to use
$$
  s\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}, \ \ \ \text{where} \ \ \ s = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}},
$$
and the degrees of freedom becomes $n_1 + n_2 - 2$.

Why would we use this alternative standard error? 
