---
output:
  html_document:
    theme: readable
---

```{r, echo = FALSE, message = FALSE}
library(lubridate)
date <- "09-29-2021"
weekday <- wday(mdy(date), label = TRUE, abbr = FALSE)
month <- month(mdy(date), label = TRUE)
day <- day(mdy(date))
```

---
title: `r paste(weekday, ", ", month, " ", day, sep = "")`
header-includes:
  - \usepackage{array}
  - \usepackage{booktabs}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, fig.align = "center", out.width = "100%", fig.width = 9, cache = FALSE, dev = ifelse(knitr::is_html_output(), "png", "pdf"))
```

```{r packages}
library(dplyr)
library(tidyr)
library(ggplot2)
library(kableExtra)
```

```{r utilities}
source("../../utilities.R")
```

`r ifelse(knitr::is_html_output(), paste("You can also download a [PDF](lecture-", date, ".pdf) copy of this lecture.", sep = ""), "")`


## The "Anatomy" of Confidence Intervals

Many confidence intervals (and all that will be discussed in this class) have the form
$$
\large
\overbrace{\text{point estimate} \pm \underbrace{\text{standard score} \times \text{standard error}}_{\text{margin of error}}}^{\text{confidence interval}}.
$$

## Confidence Level

The **confidence level** of a confidence interval formula is the probability an interval produced by the formula will contain the parameter *before the data are collected* (after the data are collected the interval either does or does not contain the parameter). It is controlled through the *standard score*.

**Example**: Recall the study with the platies. Out of a sample of 84 observations, the yellow-tailed male was preferred on 67 observations. Let $p$ be the *probability* of a preference for the yellow-tailed male. Let $p$ be the probability that a female will prefer to the yellow-tailed male. What is our estimate of $p$ using the confidence interval
$$
  \hat{p} \pm z\sqrt{\hat{p}(1-\hat{p})/n},
$$
with a *confidence level* of 95%? 

\vspace{3cm}

How about a confidence level of 90% or 99%? Note that we can look up $z$ for *any* desired confidence level using [statdistributions.com](http://www.statdistributions.com).
```{r}
d <- data.frame(Level = c(0.68, 0.9, 0.95, 0.99))
d$z <- with(d, -round(qnorm((1-d$Level)/2), 3))
if (knitr::is_html_output()) {
  d$Level <- paste(d$Level*100, "%", sep = "")
} else {
  d$Level <- paste(d$Level*100, "\\%", sep = "")
}
  names(d)[2] <- "$z$"
ktbl(d)
```

\pagebreak

Suppose that $p$ = 0.8. What would happen if we repeated the study many times over, each time computing a confidence interval 
$$
  \hat{p} \pm z\sqrt{\hat{p}(1-\hat{p})/n}.
$$
to estimate $p$? Each panel below shows 100 confidence intervals.
```{r, fig.height = 8, fig.width = 9}
p <- 0.8
n <- 84
tmp <- data.frame(phat = rep(NA, 300), lowb = rep(NA, 300), 
  uppb = rep(NA, 300), hitp = rep(NA, 300), cl = rep(NA, 300))
i <- 1
for (cl in c(0.9,0.95,0.99)) {
  set.seed(123)
  for (j in 1:100) {
    y <- rbinom(n, 1, p)
    tmp$phat[i] <- mean(y)
    tmp$lowb[i] <- tmp$phat[i] + qnorm((1 - cl)/2) * sqrt(tmp$phat[i] * (1 - tmp$phat[i]) / n)
    tmp$uppb[i] <- tmp$phat[i] - qnorm((1 - cl)/2) * sqrt(tmp$phat[i] * (1 - tmp$phat[i]) / n)
    tmp$hitp[i] <- (p < tmp$uppb[i]) * (p > tmp$lowb[i])
    tmp$cl[i] <- cl
    i <- i + 1
  }
} 

tmp$sample <- rep(1:100, 3)
if (knitr::is_html_output()) {
  tmp$cl <- factor(tmp$cl, labels = paste("Confidence Level:", c("90%","95%","99%")))
} else {
  tmp$cl <- factor(tmp$cl, labels = paste("Confidence Level:", c("90\\%","95\\%","99\\%")))
}
tmp$hitp <- factor(tmp$hitp, labels = c("no","yes"))

p <- ggplot(tmp, aes(x = sample, y = phat, color = hitp))
p <- p + geom_hline(yintercept = 0.8, color = grey(0.5), size = 0.5)
p <- p + geom_point(size = 1)
p <- p + geom_linerange(aes(ymin = lowb, ymax = uppb), size = 0.25)
p <- p + xlab("Sample")
p <- p + ylab(ifelse(knitr::is_html_output(), "p", "$p$"))
p <- p + labs(color = "Correct?") + ylim(0.5,1)
p <- p + scale_color_manual(values = c("black", grey(0.75)))
p <- p + theme_minimal()
p <- p + facet_grid(cl ~ .) + guides(color = "none")
plot(p)
```

\pagebreak

```{r, fig.height = 8, fig.width = 9}
set.seed(101)
p <- 0.8
cl <- 0.95
tmp <- data.frame(phat = rep(NA, 300), lowb = rep(NA, 300), 
  uppb = rep(NA, 300), hitp = rep(NA, 300), n = rep(NA, 300))
i <- 1
for (n in c(50,100,400)) {
  for (j in 1:100) {
    y <- rbinom(n, 1, p)
    tmp$phat[i] <- mean(y)
    tmp$lowb[i] <- tmp$phat[i] + qnorm((1 - cl)/2) * sqrt(tmp$phat[i] * (1 - tmp$phat[i]) / n)
    tmp$uppb[i] <- tmp$phat[i] - qnorm((1 - cl)/2) * sqrt(tmp$phat[i] * (1 - tmp$phat[i]) / n)
    tmp$hitp[i] <- (p < tmp$uppb[i]) * (p > tmp$lowb[i])
    tmp$n[i] <- n
    i <- i + 1
  }
} 

tmp$sample <- rep(1:100, 3)
tmp$n <- factor(tmp$n, labels = paste("Sample Size:", c(50,100,400)))
tmp$hitp <- factor(tmp$hitp, labels = c("no","yes"))

p <- ggplot(tmp, aes(x = sample, y = phat, color = hitp))
p <- p + geom_hline(yintercept = 0.8, color = grey(0.5), size = 0.5)
p <- p + geom_point(size = 1)
p <- p + geom_linerange(aes(ymin = lowb, ymax = uppb), size = 0.25)
p <- p + xlab("Sample")
p <- p + ylab(ifelse(knitr::is_html_output(), "p", "$p$"))
p <- p + labs(color = "Correct?") + ylim(0.5,1)
p <- p + scale_color_manual(values = c("black", grey(0.75)))
p <- p + theme_minimal()
p <- p + facet_grid(n ~ .) + guides(color = "none")
plot(p)
```

\pagebreak

How does increasing the *confidence level* affect the margin of error and confidence interval?

How does increasing the *sample size* affect the margin of error and confidence interval?

Suppose I obtain 1000 samples, and from each sample I computed a confidence interval to estimate $p$ using the formula
$$
  \hat{p} \pm 1.96\sqrt{\hat{p}(1-\hat{p})/n}.
$$
Approximately how many samples would contain $p$? What if we replaced 1.96 with 2.576?

\pagebreak

## Confidence Intervals for $\mu$

The *actual* confidence level of the confidence interval 
$$
  \bar{x} \pm zs/\sqrt{n}
$$
is less than the *specified* confidence level, particularly if $n$ is small.

```{r, fig.height = 3, fig.width = 9}
set.seed(123)

n <- 5
cl <- 0.95
mu <- 50
si <- 5

tmp <- data.frame(xbar = rep(NA, 100))

for (i in 1:100) {
  y <- rnorm(n, mu, si)
  s <- sd(y)
  tmp$xbar[i] <- mean(y)
  tmp$lowb[i] <- tmp$xbar[i] + qnorm((1 - cl)/2) * s / sqrt(n) 
  tmp$uppb[i] <- tmp$xbar[i] - qnorm((1 - cl)/2) * s / sqrt(n)
  tmp$hitp[i] <- (mu < tmp$uppb[i]) * (mu > tmp$lowb[i])
}

tmp$sample <- 1:100
tmp$hitp <- factor(tmp$hitp, labels = c("no","yes"))

musym <- expression(mu)

p <- ggplot(tmp, aes(x = sample, y = xbar, color = hitp))
p <- p + geom_hline(yintercept = mu, color = grey(0.5), size = 0.5)
p <- p + geom_point(size = 1)
p <- p + geom_linerange(aes(ymin = lowb, ymax = uppb), size = 0.25)
p <- p + xlab("Sample")
p <- p + ylab(ifelse(knitr::is_html_output(), expression(mu), "$\\mu$"))
p <- p + labs(color = "Correct?")
p <- p + scale_color_manual(values = c("black", grey(0.75)))
p <- p + theme_minimal() + guides(color = "none")
plot(p)
```

A solution is to modify the confidence interval as
$$
  \bar{x} \pm ts/\sqrt{n},
$$
where $t$ is a ["t-score" from the $t$-distribution](http://www.statdistributions.com) with degrees of freedom $n-1$.

```{r, fig.height = 3, fig.width = 9}
set.seed(123)

n <- 5
cl <- 0.95
mu <- 50
si <- 5

tmp <- data.frame(xbar = rep(NA, 100))

for (i in 1:100) {
  y <- rnorm(n, mu, si)
  s <- sd(y)
  tmp$xbar[i] <- mean(y)
  tmp$lowb[i] <- tmp$xbar[i] + qt((1 - cl)/2, df = n - 1) * s / sqrt(n) 
  tmp$uppb[i] <- tmp$xbar[i] - qt((1 - cl)/2, df = n - 1) * s / sqrt(n)
  tmp$hitp[i] <- (mu < tmp$uppb[i]) * (mu > tmp$lowb[i])
}

tmp$sample <- 1:100
tmp$hitp <- factor(tmp$hitp, labels = c("no","yes"))

musym <- expression(mu)

p <- ggplot(tmp, aes(x = sample, y = xbar, color = hitp))
p <- p + geom_hline(yintercept = mu, color = grey(0.5), size = 0.5)
p <- p + geom_point(size = 1)
p <- p + geom_linerange(aes(ymin = lowb, ymax = uppb), size = 0.25)
p <- p + xlab("Sample")
p <- p + ylab(ifelse(knitr::is_html_output(), expression(mu), "$\\mu$"))
p <- p + labs(color = "Correct?")
p <- p + scale_color_manual(values = c("black", grey(0.75)))
p <- p + theme_minimal() + guides(color = "none")
plot(p)
```

**Example**: Consider the following data from a study of the volume of the left hippocampus for twin pairs discordant for schizophrenia.[^schizo]
```{r}
d <- Sleuth3::case0202
d$Difference <- with(d, Unaffected - Affected)
m <- round(mean(d$Difference), 2)
s <- round(sd(d$Difference), 2)
n <- nrow(d)
d$Pair = 1:n
d <- d[,c(4,1,2,3)]
ktbl(headtail(round(d,2), 5)) %>% add_header_above(c(" " = 1, "Twin" = 2, " " = 1))
```
```{r, fig.height = 1.5}
d <- Sleuth3::case0202
d$pair <- factor(1:nrow(d))
d <- reshape2::melt(d, measure.vars = c("Unaffected","Affected"),
  variable.name = "twin", value.name = "volume")

tmp <- d %>% group_by(twin) %>% summarize(mean = mean(volume))
tmp$x <- 1:2

p <- ggplot(d, aes(x = twin)) + coord_flip() + theme_classic()
p <- p + labs(x = "", y = "Left Hippocampus Volume")
p <- p + geom_dotplot(aes(y = volume), binaxis = "y", stackdir = "up", binwidth = 0.01)
p <- p + geom_line(aes(x = as.numeric(twin) + 0.03, y = volume, group = pair), color = grey(0.5))
p <- p + geom_dotplot(aes(y = volume), binaxis = "y", stackdir = "up", binwidth = 0.01)
p <- p + geom_point(aes(y = mean, x = x - 0.1), data = tmp, pch = 17)
p <- p + ggtitle("Distribution of Volume by Twin Status")
p1 <- p

d <- Sleuth3::case0202
d$diff <- with(d, Unaffected - Affected)
d$m <- with(d, mean(Unaffected - Affected))

p <- ggplot(d, aes(x = Unaffected - Affected)) + theme_classic() + noyaxis
p <- p + geom_dotplot(binwidth = 0.01)
p <- p + labs(x = "Difference in Left Hippocampus Volume")
p <- p + geom_point(aes(x = m, y = -0.1), pch = 17)
p <- p + ggtitle("Distribution of Difference of Volume")
p2 <- p

#cowplot::plot_grid(p1, p2, align = "v", ncol = 1, rel_heights = c(2,1))
plot(p2)
```

The mean difference from the sample is $\bar{x}$ = `r m` cubic cm, and the standard deviation from the sample is $s$ = `r s` cubic cm. Let $\mu$ be the mean difference in volume for the probability distribution of one observation of the difference in mean volume. What are the point estimate, margin of error, and confidence interval (with a confidence level of 95%) for estimating $\mu$?

[^schizo]: Suddath, R. L., Christison, G. W., Torrey, E. F., Casanova, M. F., & Weinberger, D. R. (1990). Anatomical abnormalities in the brains of monozygotic twins discordant for schizophrenia. *New England Journal of Medicine*, *322*, 789--794. 

\pagebreak

**Example**: Recall the study of beak length of finches on Daphne Major in 1976 and 1978.[^grant]
```{r, fig.height = 3}
d <- Sleuth3::ex0218
tmp <- d %>% group_by(Year) %>% summarize(mean = round(mean(Depth),1), sd = round(sd(Depth),1), n = n())
p <- ggplot(d, aes(x = 0, y = Depth)) + coord_flip() + facet_wrap(~ Year)
p <- p + geom_dotplot(binaxis = "y", binwidth = 0.1, stackdir = "up")
p <- p + theme(axis.title.y = element_blank(), axis.text.y = element_blank(),
  axis.ticks.y = element_blank()) + labs(y = "Beak Depth (mm)") + theme_minimal() + noyaxis
p <- p + geom_point(aes(y = mean, x = -0.05), data = tmp, shape = 17, size = 2)
plot(p)
```
Let $\mu_{76}$ and $\mu_{78}$ be the means of the distributions of beak length in 1976 and 1978, respectively (i.e., the mean beak length of *all* finches on the island those years). What are the point estimates, margins of error, and confidence intervals for $\mu_{76}$ and $\mu_{78}$?  
```{r}
names(tmp) <- c("Year","$\\bar{x}$", "$s$", "$n$")
ktbl(tmp)
```

**Important**: From now on we will not necessarily being using 2 as our standard score in confidence intervals. For confidence intervals for $p$, look up the value of $z$ corresponding to the desired confidence level. For confidence intervals for $\mu$, look up the value of $t$ corresponding to the desired confidence level *and* degrees of freedom ($n-1$). 

[^grant]: Grant, P. (1986). *Ecology and evolution of Darwin's finches*. Princeton, N.J.: Princeton University Press.
