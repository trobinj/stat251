---
output:
  html_document:
    theme: readable
---

```{r, echo = FALSE, message = FALSE}
library(lubridate)
date <- "09-20-2021"
weekday <- wday(mdy(date), label = TRUE, abbr = FALSE)
month <- month(mdy(date), label = TRUE)
day <- day(mdy(date))
```

---
title: `r paste(weekday, ", ", month, " ", day, sep = "")`
header-includes:
  - \usepackage{array}
  - \usepackage{float}
  - \usepackage{booktabs}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, fig.align = "center", out.width = "100%", fig.width = 9, cache = FALSE, dev = ifelse(knitr::is_html_output(), "png", "pdf"))
```

```{r packages}
library(dplyr)
library(tidyr)
library(ggplot2)
library(kableExtra)
```

```{r utilities}
source("../../utilities.R")
```

`r ifelse(knitr::is_html_output(), paste("You can also download a [PDF](lecture-", date, ".pdf) copy of this lecture.", sep = ""), "")`

## Sampling Distributions of $\bar{x}$ and $\hat{p}$

1. The mean of $\bar{x}$ equals $\mu_x$ (i.e., $\mu_{\bar{x}} = \mu_x$). The standard deviation of $\bar{x}$ is $\sigma_x/\sqrt{n}$ (i.e., $\sigma_{\bar{x}} = \sigma_x/\sqrt{n}$). 

2. The mean of $\hat{p}$ equals $p$ (i.e., $\mu_{\hat{p}} = p$). The standard deviation of $\hat{p}$ is $\sqrt{p(1-p)/n}$ (i.e., $\sigma_{\hat{p}} = \sqrt{p(1-p)/n}$). 

## Proportions are Means

Suppose we have a population distribution for successes and failures, but we define a random variable $x$ so that $x$ = 1 if we observe a success, and $x$ = 0 if we observe a failure. 
```{r}
d <- data.frame(x = c(1,0), Probability = c("$p$","$1-p$"))
names(d) <- c("$x$","$P(x)$")
ktbl(d)
```
The mean of $x$ is
$$
  \mu_x = \sum xP(x) = 1 \times p + 0 \times (1-p) = p,
$$
and the standard deviation of $x$ is
$$
  \sigma_x = \sqrt{\sum (x - \mu)^2P(x)} = 
  \sqrt{(1-p)^2 \times p + (0-p)^2 \times (1-p)} = \sqrt{p(1-p)}.
$$
The mean of a sample of observations of $x$ (i.e., $\bar{x}$) *is a proportion*. For example, if our observations of $x$ are 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, then the mean is
$$
  \bar{x} = \frac{1 + 1 + 0 + 1 + 0 + 0 + 0 + 1 + 1 + 1}{10} = 0.6,
$$
which is also the *proportion of observations where we observe a success*. So $\hat{p} = \bar{x}$. 

Now applying what we know about the sampling distribution of $\bar{x}$, we have
\begin{align*}
  \mu_{\bar{x}} & = \mu_x = p, \\
  \sigma_{\bar{x}} & = \sigma_x/\sqrt{n} = \sqrt{p(1-p)}/\sqrt{n} = \sqrt{p(1-p)/n}. 
\end{align*}

## Central Limit Theorem

**Central Limit Theorem**: If $X_1, X_2, \dots, X_n$ are independently and identically distributed random variables so that $E(X_i) = \mu$ and $E(X_i - \mu)^2 = \sigma^2 < \infty$, then
$$
\sqrt{n}(\bar{X} - \mu)/\sigma \stackrel{d}{\rightarrow} N(0,1),
$$
where $\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i$, and where $\stackrel{d}{\rightarrow}$ denotes convergence in distribution.

**Central Limit Theorem** (Layperson's Version): As $n$ increases, the *shape* of the sampling distribution of $\bar{x}$ "approaches" that of a normal distribution. 

**Example**: Suppose we roll $n$ = 2 fair 6-sided dice. What is the shape of the sampling distribution of the *mean* number of dots (i.e., $\bar{x}$)?
```{r}
d <- data.frame(x = 1:6, Probability = rep("1/6", 6))
names(d) <- c("$x$","$P(x)$")
ktbl(d, caption = "Population Distribution")
d <- cbind(1:6, outer(1:6, 1:6, function(a,b) a + b)/2)
d <- data.frame(d)
names(d) <- c("First Die", 1:6)
ktbl(d, caption = "Sample Space") %>% column_spec(1, bold = TRUE) %>% add_header_above(c(" " = 1, "Second Die" = 6))
```
Note: Because each side has probability 1/6, the probability of each sample is 1/6 $\times$ 1/6 = 1/36.
```{r, fig.height = 3, fig.width = 9}
d <- data.frame(x = seq(1, 6, by = 0.5), p = paste(c(1,2,3,4,5,6,5,4,3,2,1), "/", 36, sep = ""))
names(d) <- c("Mean","Probability")
names(d) <- c("$\\bar{x}$","$P(\\bar{x})$")
ktbl(d, caption = "Sampling Distribution")

d <- data.frame(x = seq(1, 6, by = 0.5), p = c(1,2,3,4,5,6,5,4,3,2,1)/36)
p <- ggplot(d, aes(x = x, y = p)) + geom_point() + geom_segment(aes(xend = x, yend = 0))
p <- p + theme_minimal() + labs(x = tex("$\\bar{x}$"), y = "Probability") + ggtitle("Sampling Distribution")
p <- p + scale_x_continuous(breaks = unique(d$x))
plot(p)
```

What if we roll $n$ = 3, $n$ = 5, or $n$ = 7 dice?

```{r, fig.width = 9}
g <- function(x, p, n, stat = mean) {
  ytmp <- vector(mode = "list", n)
  ptmp <- vector(mode = "list", n)
  for (i in 1:n) {
    ytmp[[i]] <- x
    ptmp[[i]] <- p
  }
  ytmp <- apply(expand.grid(ytmp), 1, stat)
  ptmp <- apply(expand.grid(ptmp), 1, prod)
  data.frame(x = ytmp, p = ptmp, n = n) %>% group_by(x, n) %>% summarize(p = sum(p))
}

d3 <- g(1:6, rep(1/6, 6), 3)
d4 <- g(1:6, rep(1/6, 6), 5)
d5 <- g(1:6, rep(1/6, 6), 7)

d <- rbind(d3,d4,d5)

d$n <- paste("n = ", d$n, sep = "")

p <- ggplot(d, aes(x = x, y = p)) + facet_grid(n ~ .)
p <- p + geom_point() + geom_segment(aes(xend = x, yend = 0))
p <- p + theme_minimal() + labs(x = tex("$\\bar{x}$"), y = "Probability")
p <- p + scale_x_continuous(breaks = 1:6)
p <- p + ggtitle("Sampling Distributions")
plot(p)
```
Here is [another demonstration](http://onlinestatbook.com/stat_sim/sampling_dist) that uses simulation to illustrate the central limit theorem. 

The *practical* implication of the central limit theorem is that we can often assume that the shape of the sampling distribution of $\bar{x}$ (or $\hat{p}$) is approximately that of a normal distribution.

## Applying the Central Limit Theorem

Recall that the empirical rule states that *approximately 95% of observations are within two standard deviations of the mean*. Adapting this to a *normal probability distribution*, we can say that *there is approximately a probability of 0.95 that the random variable will be within two standard deviations of the mean of the distribution*. 

1. The probability that $\bar{x}$ will be between $\mu_x - 2\sigma_x/\sqrt{n}$ and $\mu_x + 2\sigma_x/\sqrt{n}$ is approximately 0.95.

0. The probability that $\hat{p}$ will be between $p - 2\sqrt{p(1-p)/n}$ and $p + 2\sqrt{p(1-p)/n}$ is approximately 0.95. 

\pagebreak

**Example**: Recall Darwin's experiment that compared cross-fertilization and self-fertilization. 
```{r}
d <- HistData::ZeaMays %>% mutate(Obs = 1:n()) %>% select(Obs, cross, self, diff) %>% 
  rename(Cross = cross, Self = self, Difference = diff) %>% 
  mutate(Cross = format(Cross, nsmall = 3), Self = format(Self, nsmall = 3), 
         Difference = format(Difference, nsmall = 3))
ktbl(headtail(d,5)) %>% add_header_above(c(" " = 1, "Fertilization" = 2, " " = 1))
```

1. Let $x$ be the *difference* in height for a given pair of seedlings. *Assume* that $x$ has a mean of 3 and a standard deviation of 5. So $\mu_x$ = 3, but Darwin didn't know that. So he'd use $\bar{x}$ to estimate $\mu_x$. How close might it be? What are the mean and standard deviation of $\bar{x}$ based on a sample of $n$ = 15 observations? What is the interval that has a probability of approximately 0.95 of containing $\bar{x}$? 

\vspace{3cm}

2. Let $x$ be which seedling is taller, with the following *population distribution*.
```{r}
d <- data.frame(x = c("cross","self"), p = c(0.8,0.2))
names(d) <- c("$x$","$P(x)$")
ktbl(d)
```
So here the seedling produced by cross-fertilization is more likely to be taller than one produced by self-fertilization. This would be useful to know, but Darwin didn't know the value of $p$. But he could estimate it based on a sample of observations using $\hat{p}$, the *proportion* of pairs in a sample of observations in which the seedling produced by cross-fertilization is taller. What are the mean and standard deviation of $\hat{p}$ based on a sample of $n$ = 15 observations? What is the interval that has a probability of approximately 0.95 of containing $\hat{p}$? 

\pagebreak

## Estimation

*Estimation* is a kind of inference in which we use a statistic to estimate a parameter. 

1. We can use $\bar{x}$ (i.e., the mean of a sample of $n$ observations of $x$) to estimate the mean of a single observation (i.e., $\mu_x$).

0. We can use $\hat{p}$ (i.e., the proportion of observations in a sample of $n$ observations where we observed a "success") to estimate the probability of a "success" (i.e., $p$).

 Note: Parameters like $\mu_x$ and $p$ have a couple of interpretations here. One is that they are properties of the population distribution. But in a survey with a finite number of observations, $\mu_x$ is also the mean of *all* observations in the population, and $p$ is a proportion based on *all* observations in the population. This is because in these cases the population distribution is both a probability distribution and also the distribution of all observations in the population. 

The *sampling distributions* of $\bar{x}$ and $\hat{p}$ are what we use to determine how effective these statistics are at estimating the parameters $\mu_x$ and $p$, respectively.

1. Both $\bar{x}$ and $\hat{p}$ are **unbiased**, meaning that the mean of $\bar{x}$ equals $\mu_x$, and the mean of $\hat{p}$ equals $p$. 

2. A **standard error** is the standard deviation of a statistic. The standard error of $\bar{x}$ is $\sigma_x/\sqrt{n}$, and the standard error of $\hat{p}$ is $\sqrt{p(1-p)/n}$. 

3. The **central limit theorem** implies that (unless $n$ is very small) we can regard the *shape* of the sampling distributions of $\bar{x}$ and $\hat{p}$ as approximately that of a normal distribution for the purpose of computing probabilities concerning $\bar{x}$ or $\hat{p}$.